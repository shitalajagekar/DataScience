{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "--2019-08-25 05:24:11--  http://spamham.txt/\nResolving spamham.txt (spamham.txt)... failed: Name or service not known.\nwget: unable to resolve host address \u2018spamham.txt\u2019\n--2019-08-25 05:24:11--  https://raw.githubusercontent.com/vincechan/spam-detection/master/data/SpamDetectionData.txt\nResolving raw.githubusercontent.com (raw.githubusercontent.com)... 199.232.8.133\nConnecting to raw.githubusercontent.com (raw.githubusercontent.com)|199.232.8.133|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 3602319 (3.4M) [text/plain]\nSaving to: \u2018SpamDetectionData.txt\u2019\n\n100%[======================================>] 3,602,319   --.-K/s   in 0.1s    \n\n2019-08-25 05:24:12 (35.1 MB/s) - \u2018SpamDetectionData.txt\u2019 saved [3602319/3602319]\n\nFINISHED --2019-08-25 05:24:12--\nTotal wall clock time: 0.5s\nDownloaded: 1 files, 3.4M in 0.1s (35.1 MB/s)\n"
                }
            ],
            "source": "!wget spamham.txt https://raw.githubusercontent.com/vincechan/spam-detection/master/data/SpamDetectionData.txt"
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "total no. of samples: 2100\ntotal no. of spam samples: 1043\ntotal no. of ham samples: 1057\n\nPrint a random sample for inspection:\nexample feature: <p>Harold prose and from he maddest deem finds will were mine and he from. To spoiled for pile loved deem power he dwelt sadness longed one made long these sister that and oft. One was carnal below in losel. Worse spent for name within now it and deadly and to though have she. He vaunted whence native heralds left in there into in. Parasites reverie day evil dear now favour shamed and come the feud.</p><p>Concubines flaunting uncouth had smile mirthful yes soul a few days the but few carnal had revellers. To partings were and known be paphian woe not he. Condole sighed little glee he weary florid will forgot now seek had all scene time sore deemed that low.</p>\nexample label: 1 (spam)\n"
                }
            ],
            "source": "import random\n\ndef read_file(path):\n    \"\"\"\n    read and return all data in a file\n    \"\"\"\n    with open(path, 'r') as f:\n        return f.read()\n\ndef load_data():\n    \"\"\"\n    load and return the data in features and labels lists\n    each item in features contains the raw email text\n    each item in labels is either 1(spam) or 0(ham) and identifies corresponding item in features\n    \"\"\"\n    # load all data from file\n    data_path = \"SpamDetectionData.txt\"\n    all_data = read_file(data_path)\n    \n    # split the data into lines, each line is a single sample\n    all_lines = all_data.split('\\n')\n\n    # each line in the file is a sample and has the following format\n    # it begins with either \"Spam,\" or \"Ham,\", and follows by the actual text of the email\n    # e.g. Spam,<p>His honeyed and land....\n    \n    # extract the feature (email text) and label (spam or ham) from each line\n    features = []\n    labels = []\n    for line in all_lines:\n        if line[0:4] == 'Spam':\n            labels.append(1)\n            features.append(line[5:])\n            pass\n        elif line[0:3] == 'Ham':\n            labels.append(0)\n            features.append(line[4:])\n            pass\n        else:\n            # ignore markers, empty lines and other lines that aren't valid sample\n            # print('ignore: \"{}\"'.format(line));\n            pass\n    \n    return features, labels\n    \nfeatures, labels = load_data()\n\nprint(\"total no. of samples: {}\".format(len(labels)))\nprint(\"total no. of spam samples: {}\".format(labels.count(1)))\nprint(\"total no. of ham samples: {}\".format(labels.count(0)))\n\nprint(\"\\nPrint a random sample for inspection:\")\nrandom_idx = random.randint(0, len(labels))\nprint(\"example feature: {}\".format(features[random_idx][0:]))\nprint(\"example label: {} ({})\".format(labels[random_idx], 'spam' if labels[random_idx] else 'ham'))"
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "no. of train features: 1680\nno. of train labels: 1680\nno. of test features: 420\nno. of test labels: 420\n"
                }
            ],
            "source": "from sklearn.model_selection import train_test_split\n\n# load features and labels\nfeatures, labels = load_data()\n\n# split data into training / test sets\nfeatures_train, features_test, labels_train, labels_test = train_test_split(\n    features, \n    labels, \n    test_size=0.2,   # use 10% for testing\n    random_state=42)\n\nprint(\"no. of train features: {}\".format(len(features_train)))\nprint(\"no. of train labels: {}\".format(len(labels_train)))\nprint(\"no. of test features: {}\".format(len(features_test)))\nprint(\"no. of test labels: {}\".format(len(labels_test)))"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## Bag of words"
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "['hello, how are you!', 'win money, win from home.', 'call me now.', 'hello, call hello you tomorrow?']\n"
                }
            ],
            "source": "documents = ['Hello, how are you!',\n             'Win money, win from home.',\n             'Call me now.',\n             'Hello, Call hello you tomorrow?']\n\nlower_case_documents = []\nlower_case_documents = [d.lower() for d in documents]\nprint(lower_case_documents)"
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "['hello how are you',\n 'win money win from home',\n 'call me now',\n 'hello call hello you tomorrow']"
                    },
                    "execution_count": 6,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "sans_punctuation_documents = []\nimport string\n\nfor i in lower_case_documents:\n    sans_punctuation_documents.append(i.translate(str.maketrans(\"\",\"\", string.punctuation)))\n    \nsans_punctuation_documents"
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "[['hello', 'how', 'are', 'you'],\n ['win', 'money', 'win', 'from', 'home'],\n ['call', 'me', 'now'],\n ['hello', 'call', 'hello', 'you', 'tomorrow']]"
                    },
                    "execution_count": 7,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "preprocessed_documents = [[w for w in d.split()] for d in sans_punctuation_documents]\npreprocessed_documents"
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "[Counter({'hello': 1, 'how': 1, 'are': 1, 'you': 1}),\n Counter({'win': 2, 'money': 1, 'from': 1, 'home': 1}),\n Counter({'call': 1, 'me': 1, 'now': 1}),\n Counter({'hello': 2, 'call': 1, 'you': 1, 'tomorrow': 1})]\n"
                }
            ],
            "source": "frequency_list = []\nimport pprint\nfrom collections import Counter\n\nfrequency_list = [Counter(d) for d in preprocessed_documents]\npprint.pprint(frequency_list)"
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.feature_extraction.text import CountVectorizer\ncount_vector = CountVectorizer()"
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "['are',\n 'call',\n 'from',\n 'hello',\n 'home',\n 'how',\n 'me',\n 'money',\n 'now',\n 'tomorrow',\n 'win',\n 'you']"
                    },
                    "execution_count": 10,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "count_vector.fit(documents)\ncount_vector.get_feature_names()"
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": "array([[1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1],\n       [0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 2, 0],\n       [0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0],\n       [0, 1, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1]])"
                    },
                    "execution_count": 11,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "doc_array = count_vector.transform(documents).toarray()\ndoc_array"
        },
        {
            "cell_type": "code",
            "execution_count": 13,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>are</th>\n      <th>call</th>\n      <th>from</th>\n      <th>hello</th>\n      <th>home</th>\n      <th>how</th>\n      <th>me</th>\n      <th>money</th>\n      <th>now</th>\n      <th>tomorrow</th>\n      <th>win</th>\n      <th>you</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>2</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>1</td>\n      <td>0</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "   are  call  from  hello  home  how  me  money  now  tomorrow  win  you\n0    1     0     0      1     0    1   0      0    0         0    0    1\n1    0     0     1      0     1    0   0      1    0         0    2    0\n2    0     1     0      0     0    0   1      0    1         0    0    0\n3    0     1     0      2     0    0   0      0    0         1    0    1"
                    },
                    "execution_count": 13,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "import pandas as pd\nfrequency_matrix = pd.DataFrame(doc_array, columns = count_vector.get_feature_names())\nfrequency_matrix"
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": "## TFIDF"
        },
        {
            "cell_type": "code",
            "execution_count": 28,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tfidf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>had</th>\n      <td>0.493562</td>\n    </tr>\n    <tr>\n      <th>little</th>\n      <td>0.493562</td>\n    </tr>\n    <tr>\n      <th>tiny</th>\n      <td>0.493562</td>\n    </tr>\n    <tr>\n      <th>house</th>\n      <td>0.398203</td>\n    </tr>\n    <tr>\n      <th>mouse</th>\n      <td>0.235185</td>\n    </tr>\n    <tr>\n      <th>the</th>\n      <td>0.235185</td>\n    </tr>\n    <tr>\n      <th>ate</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>away</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>cat</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>end</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>finally</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>from</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>of</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>ran</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>saw</th>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <th>story</th>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "            tfidf\nhad      0.493562\nlittle   0.493562\ntiny     0.493562\nhouse    0.398203\nmouse    0.235185\nthe      0.235185\nate      0.000000\naway     0.000000\ncat      0.000000\nend      0.000000\nfinally  0.000000\nfrom     0.000000\nof       0.000000\nran      0.000000\nsaw      0.000000\nstory    0.000000"
                    },
                    "execution_count": 28,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from sklearn.feature_extraction.text import TfidfVectorizer\n# this is a very toy example, do not try this at home unless you want to understand the usage differences\ndocs=[\"the house had a tiny little mouse\",\n      \"the cat saw the mouse\",\n      \"the mouse ran away from the house\",\n      \"the cat finally ate the mouse\",\n      \"the end of the mouse story\"\n     ]\n \n# settings that you use for count vectorizer will go here\ntfidf_vectorizer=TfidfVectorizer(use_idf=True)\n \n# just send in all your docs here\ntfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)\n# get the first vector out (for the first document)\nfirst_vector_tfidfvectorizer=tfidf_vectorizer_vectors[0]\n \n# place tf-idf values in a pandas data frame\ndf = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\ndf.sort_values(by=[\"tfidf\"],ascending=False)"
        },
        {
            "cell_type": "code",
            "execution_count": 31,
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tfidf</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>hote</th>\n      <td>0.704909</td>\n    </tr>\n    <tr>\n      <th>food</th>\n      <td>0.501549</td>\n    </tr>\n    <tr>\n      <th>service</th>\n      <td>0.501549</td>\n    </tr>\n    <tr>\n      <th>hotel</th>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table>\n</div>",
                        "text/plain": "            tfidf\nhote     0.704909\nfood     0.501549\nservice  0.501549\nhotel    0.000000"
                    },
                    "execution_count": 31,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": "from sklearn.feature_extraction.text import TfidfVectorizer\n# this is a very toy example, do not try this at home unless you want to understand the usage differences\ndocs=[\"hotel food service\",\n    \"hote food service\"\n     ]\n \n# settings that you use for count vectorizer will go here\ntfidf_vectorizer=TfidfVectorizer(use_idf=True)\n \n# just send in all your docs here\ntfidf_vectorizer_vectors=tfidf_vectorizer.fit_transform(docs)\n# get the first vector out (for the first document)\nfirst_vector_tfidfvectorizer=tfidf_vectorizer_vectors[1]\n \n# place tf-idf values in a pandas data frame\ndf = pd.DataFrame(first_vector_tfidfvectorizer.T.todense(), index=tfidf_vectorizer.get_feature_names(), columns=[\"tfidf\"])\ndf.sort_values(by=[\"tfidf\"],ascending=False)"
        },
        {
            "cell_type": "code",
            "execution_count": 32,
            "metadata": {},
            "outputs": [],
            "source": "from sklearn.feature_extraction.text import TfidfVectorizer\n\n# vectorize email text into tfidf matrix\n# TfidfVectorizer converts collection of raw documents to a matrix of TF-IDF features.\n# It's equivalent to CountVectorizer followed by TfidfTransformer.\nvectorizer = TfidfVectorizer(\n    input='content',     # input is actual text\n    lowercase=True,      # convert to lower case before tokenizing\n    stop_words='english' # remove stop words\n)\nfeatures_train_transformed = vectorizer.fit_transform(features_train)\nfeatures_test_transformed  = vectorizer.transform(features_test)"
        },
        {
            "cell_type": "code",
            "execution_count": 37,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "classifier accuracy 100.00%\n"
                }
            ],
            "source": "from sklearn.naive_bayes import MultinomialNB\nimport pickle\n\ndef save(vectorizer, classifier):\n    '''\n    save classifier to disk\n    '''\n    with open('model.pkl', 'wb') as file:\n        pickle.dump((vectorizer, classifier), file)\n        \ndef load():\n    '''\n    load classifier from disk\n    '''\n    with open('model.pkl', 'rb') as file:\n      vectorizer, classifier = pickle.load(file)\n    return vectorizer, classifier\n\n# train a classifier\nclassifier = MultinomialNB()\nclassifier.fit(features_train_transformed, labels_train)\n\n# save the trained model\nsave(vectorizer, classifier)\n\n# score the classifier accuracy\nprint(\"classifier accuracy {:.2f}%\".format(classifier.score(features_test_transformed, labels_test) * 100))"
        },
        {
            "cell_type": "code",
            "execution_count": 38,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "F score 1.00\n"
                }
            ],
            "source": "import numpy as np\nfrom sklearn import metrics\nprediction = classifier.predict(features_test_transformed)\nfscore = metrics.f1_score(labels_test, prediction, average='macro')\nprint(\"F score {:.2f}\".format(fscore))"
        },
        {
            "cell_type": "code",
            "execution_count": 41,
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": "\nPerform a test\nEMAIL: ['<p>Register below and join Rajesh Garg (Chief Financial Officer, Landmark Group) and Shail Khiyara (Customer Experience Officer, UiPath) to learn some amazing insights on Landmark Group\u2019s journey into digital transformation through automation, how they addressed the retail industry\u2019s complexity with RPA and find out the top-of-mind CFO challenges that automation can solve quickly.  </p>']\nThe email is HAM\n"
                }
            ],
            "source": "vectorizer, classifer = load()\n\nprint('\\nPerform a test')                    \n#email_input = 'enter your email here'\nemail_input = [\"\"\"<p>Register below and join Rajesh Garg (Chief Financial Officer, Landmark Group) and Shail Khiyara (Customer Experience Officer, UiPath) to learn some amazing insights on Landmark Group\u2019s journey into digital transformation through automation, how they addressed the retail industry\u2019s complexity with RPA and find out the top-of-mind CFO challenges that automation can solve quickly.  </p>\"\"\"]\nemail_input_transformed = vectorizer.transform(email_input)\nprediction = classifer.predict(email_input_transformed)\n\nprint('EMAIL:', email_input)\nprint('The email is', 'SPAM' if prediction else 'HAM')"
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": ""
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3.6",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.6.8"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 1
}